
1)let’s make sure all the packages are up to date. Type the following at the prompt and press Enter
sudo apt-get update
2)Check to see if your Ubuntu Linux operating system architecture is 32-bit or 64-bit, open up a terminal and run the following command below. Type/Copy/Paste: file /sbin/init 

3)Installing Java :
sudo apt-add-repository ppa:webupd8team/java
sudo apt-get update
sudo apt-get install oracle-java8-installer
sudo apt install oracle-java8-set-default
Java installation folder will be "/usr/lib/jvm"
export JAVA_HOME=/usr/lib/jvm/java-8-oracle
echo $JAVA_HOME
All Java environment variables are set in /etc/profile.d/jdk.sh, which is a file installed by oracle-java8-set-default and is read by the shell on start-up

4)Install eclipse
http://www.krizna.com/ubuntu/install-eclipse-ubuntu-14-04/

5)Install Tomcat 9
http://tecadmin.net/install-tomcat-9-on-ubuntu/
Start : sudo /opt/tomcat/bin/startup.sh
Stop: sudo /opt/tomcat/bin/shutdown.sh

6)Hadoop
Sudo su hduser  (hduser)
hduser@anveshan-All-Series:/usr/local/hadoop/sbin$ ./start-all.sh
/usr/local/hadoop/sbin/start-all.sh
/usr/local/hadoop/sbin/stop-all.sh
Sudo su hduser
Hadoop fs -ls /
hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/{loggedin user}
hdfs dfs -ls
Ex: hduser@anveshan-All-Series:/home/anveshan$ hdfs dfs -mkdir /user/hduser

6.1) If Data node is not starting :
That means you have cleared the metadata from namenode. Now the files which you have stored for running the word count are still in the datanode and datanode has no idea where to send the block reports since you formatted the namenode so it will not start. 
cat /usr/local/hadoop/etc/hadoop/hdfs-site.xml

This step is important, see where datanode's data is getting stored. It is the value associated for datanode.data.dir. For me it is /usr/local/hadoop/hadoop_data/hdfs/datanode. Open your terminal and navigate to above directory and delete the directory named current which will be there under that directory. Make sure you are only deleting the "current" directory.

sudo rm -r /usr/local/hadoop/hadoop_data/hdfs/datanode/current
hadoop namenode -format
Jps

2800 DataNode
3377 NodeManager
3219 ResourceManager

3748 Jps
2648 NameNode
3019 SecondaryNameNode

7)Install  virtual box
http://tecadmin.net/install-oracle-virtualbox-on-ubuntu/
Virtualbox

Hadoop commands:
Login to hduser for connecting to hadoop
Sudo su hduser  (hduser)
hduser@anveshan-All-Series:/usr/local/hadoop/sbin$ ./start-all.sh
/usr/local/hadoop/sbin/start-all.sh
/usr/local/hadoop/sbin/stop-all.sh
http://localhost:50070/
Jps -- to check what demons are running in the background
--Below steps to remove an error
Sudo su hduser
Hadoop fs -ls /  (if there is an error)
hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/{loggedin user}
hdfs dfs -ls
Local directory : /user/anveshan/hdpl
Hadoop directory :/user/hduser
(in cloudera, /home/cloudera is local and /user/cloudera is hadoop FS)
--copyFromLocal
hadoop fs -copyFromLocal helloworld.txt /user/hduser/
Check the urls :
http://localhost:8088/cluster resource manager
http://localhost:50070/dfshealth.html#tab-overview Name node
http://localhost:50070/logs/  logs
http://localhost:50070/logs/userlogs/  (check the application from the console)

8)Running a map reduce :
•If we are creating a executable jar, then we don’t have to mention class name
/home/anveshan$ hadoop jar /home/anveshan/hdpl/progs/wc1.jar mrprogs/wc1/ip mrprogs/wc1/op1
hadoop fs -cat /user/hduser/mrprogs/wordCount/op_cins/part-r-00000

Install Spark :
https://spark.apache.org/downloads.html
Download spark 
sudo tar xvf /usr/local/spark-2.0.1-bin-hadoop2.7.tgz
sudo mv spark-2.0.1-bin-hadoop2.7 spark
Edit  bashrc to include SPARk_HOME=/usr/local/spark
If in hduser, then got root of user folder  ..do cd ~
hduser@anveshan-All-Series:/$ sudo vi .bashrc
Or 
sudo gedit .bashrc
Use the following command for sourcing the ~/.bashrc file.
$source .bashrc

Install Scala
wget http://www.scala-lang.org/files/archive/scala-2.10.4.tgz
sudo tar xvf scala-2.10.4.tgz
sudo mv scala-2.10.4 scala
-> to modify bashrc file, better do cd ~ and then sudo gedit .bashrc
sudo gedit ~/.bashrc
export SCALA_HOME=/usr/local/scala
export PATH=$PATH:$SCALA_HOME/bin
export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$SPARK_HOME/bin
Check scala
scala
Scala -version (2.10.4)

Spark :
Spark-shell
Sc.version (2.0.1)
Give this permission to hduser
sudo chown -R hduser /usr/local/spark

To start master in spark :
/usr/local/spark/sbin/start-all.sh
/usr/local/spark/sbin/start-master.sh     -- This will run the spark server..tomcat
/usr/local/spark/sbin/stop-all.sh

Sample comands :
spark-shell
:quit

Running simple prog  in spark-shell :
sudo su hduser
/usr/local/spark/sbin/start-master.sh
/usr/local/spark/sbin/start-all.sh
cd /home/anveshan/spark
spark-shell(local mode : http://localhost:4040)
Create input file : ip.txt
val inputfile = sc.textFile("ip.txt")
val counts = inputfile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_+_);
counts.saveAsTextFile("output")
In another terminal,
cd /home/anveshan/spark/output
cat part-00000
:quit

Running SPARK PROGRAM via SBT
sudo su hduser
1.Create project in spark workspace : /home/anveshan/spark/sparkHello
2.Create sbt file :
name := "Spark Hello"
version := "1.0"
scalaVersion := "2.10.4"
libraryDependencies += "org.apache.spark" %% "spark-core" % "1.1.1"
3.Create structure : src/main/scala
4.In the scala folder,create scala class to be run
5.Go spark project in cmd (/home/anveshan/spark/sparkHello)
6.sbt
7.compile
8.package
9.Now exit from sbt
10. Run the spark engine :   /usr/local/spark/sbin/start-all.sh
11. Check if engine is running : http://anveshan-all-series:8080/
12. Now submit the jar for running:
$SPARK_HOME/bin/spark-submit --class "SparkHello" --master spark://anveshan-All-Series:7077 /home/anveshan/spark/sparkHello/target/scala-2.10/spark-hello_2.10-1.0.jar
13. Stop spark : /usr/local/spark/sbin/stop-all.sh

Use
sbt compile
sbt package

Another way :
1.Start spark server
2.spark-shell --master spark://anveshan-All-Series:7077
3.open http://localhost:8080 and see that in the spark applications, spark-shell is running
4.  val v1 = sc.parallelize(1 to 1000)
5.  val v2 = sc.parallelize(1001 to 1000000)
6.  val v3 = v1.union(v2)
http://localhost:4040

http://www.infoobjects.com/spark-submit-with-sbt/
http://blog.matthewrathbone.com/2013/01/05/a-quick-guide-to-hadoop-map-reduce-frameworks.html
Running Scala  progs
============================================
# sbt installation

echo "deb https://dl.bintray.com/sbt/debian /" | sudo tee -a /etc/apt/sources.list.d/sbt.list
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 642AC823
sudo apt-get update
sudo apt-get install sbt
-- To check the version of sbt
sbt sbt-version

1.Go to Scala workplace and then to project(specific project you have written)
2.sbt
3.write one build.sbt
4.write *.scala which you program
5.run
6.select what you want to run

--Another way of running sbt with command line args
sbt "run-main Hello try"   -- run-main is command option,Hello is object name,try - is CLI arg

$ wget https://raw.github.com/snim2/gedit-scala-plugin/master/install.sh
$ chmod +x install.sh
$ ./install.sh
============================================
https://www.youtube.com/watch?v=OMo--kfkjyU&list=PLbu9W4c-C0iB--RNIzTXVksxQEI4vLK5-&index=8
https://github.com/dgadiraju/code/blob/master/hadoop/edw/cloudera/sqoop/sqoop_demo.txt
https://hadooptutorial.wikispaces.com/Custom+partitioner
============================================

This example illustrates how to use customized partitioner in a MapReduce program. The partitioning phase takes place after the map phase and before the reduce phase. The number of partitions is equal to the number of reducers. The data gets partitioned across the reducers according to the partitioning function[1] . The difference between a partitioner and a combiner is that the partitioner divides the data according to the number of reducers so that all the data in a single partition gets executed by a single reducer. However, the combiner functions similar to the reducer and processes the data in each partition. The combiner is an optimization to the reducer. The default partitioning function is the hash partitioning function where the hashing is done on the key. However it might be useful to partition the data according to some other function of the key or the value.

Spark Programs :
val sampleFile = sc.textFile("sample.txt")
val linesWithSample = sampleFile.filter(line => line.contains("sample"))
linesWithSample.count()
